{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70d6010",
   "metadata": {},
   "source": [
    "# Training a Single Model for Computer Vision and Natural Language Processing\n",
    "\n",
    "In this notebook, we train a single MANN model to perform both a computer vision task (image classification) as well as a natural language processing task (sentiment analysis).  For this notebook, we will use the MNIST Fashion dataset as well as the IMDB Sentiment Analysis dataset to train and test on.  Training will be done in a sequential manner, i.e. the network will first learn to perform image classification and then will learn to perform sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7adf8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 14:23:03.476937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:03.476971: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Load the packages required for the experiment\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646127f8",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Data\n",
    "\n",
    "Our first step in this demonstration will be to load the data.  We are using the TensorFlow functions to download and/or load the datasets into memory.\n",
    "\n",
    "The `Fashion MNIST` dataset, which can be found at [this link](https://github.com/zalandoresearch/fashion-mnist), contains 60,000 grayscale training images of clothing items from 10 different classes, each 28 pixels high and 28 pixels wide.  The test set contains 10,000 grayscale images from the same 10 classes and with the same shape.  The only preprocessing we will be doing is dividing all pixel values by 255 to ensure all values fall in the interval [0, 1].\n",
    "\n",
    "The `IMDB Large Movie Review` dataset, which can be found at [this link](https://ai.stanford.edu/~amaas/data/sentiment/), contains 25,000 \"highly polar\" movie reviews for training and 25,000 reviews for testing.  When loaded through the TensorFlow function, these reviews are already converted into sequences of integers, where each integer corresponds to an individual word.  For our purposes, we will restrict the vocabulary to 10,000 words, so only the top 10,000 words will be represented, and any other words will be left as \"out of vocabulary\" (OOV).  Once that is complete, we will preprocess the inputs so that only the first 500 words in each review are considered, truncated and padded from the rear of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d50de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(fashion_x_train, fashion_y_train), (fashion_x_test, fashion_y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(imdb_x_train, imdb_y_train), (imdb_x_test, imdb_y_test) = tf.keras.datasets.imdb.load_data(num_words = 10000)\n",
    "\n",
    "# Preprocess each of the input datasets. For the images, normalize each of the pixels to values between 0 and 1.\n",
    "# For the reviews, truncate and/or pad the lengths to 500 words each\n",
    "fashion_x_train = fashion_x_train/255\n",
    "fashion_x_test = fashion_x_test/255\n",
    "imdb_x_train = tf.keras.preprocessing.sequence.pad_sequences(imdb_x_train, maxlen = 500, padding = 'post', truncating = 'post')\n",
    "imdb_x_test = tf.keras.preprocessing.sequence.pad_sequences(imdb_x_test, maxlen = 500, padding = 'post', truncating = 'post')\n",
    "\n",
    "# Reshape the target data to having one column\n",
    "fashion_y_train = fashion_y_train.reshape(-1, 1)\n",
    "fashion_y_test = fashion_y_test.reshape(-1, 1)\n",
    "imdb_y_train = imdb_y_train.reshape(-1, 1)\n",
    "imdb_y_test = imdb_y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcf698",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "Now that the data has been loaded and preprocessed, we can create the model.  Because of the different input shapes for our inputs, we have to develop some dedicated input regions within the model.  After data flows through those regions, which are pruned along with the rest of the network and include only a flatten and single fully connected layer for the fashion data and an embedding and fully connected layer for the IMDB data, the bulk of the network consists of shared layers.  \n",
    "\n",
    "In these shared layers, the training procedure identifies disjoint subsets of weights to dedicate to each individual task.  By isolating these highly-pruned subnetworks and representing the network weights across an extra, task-specific dimension, the network is able to learn multiple tasks in parallel or in sequence without exhibiting any interference between tasks.\n",
    "\n",
    "After passing through the shared layers, there are dedicated output layers for each task.  These are required because of the different output shapes between tasks - the image classification task differentiates between 10 different classes, whereas the sentiment analysis task performs binary classification to identify positive and negative reviews.\n",
    "\n",
    "After the model is instantiated, we utilize the `mask_model` function to perform masking (aka pruning) of the model.  This identifies the isolated subnetworks to perform each individual task and prepares the network for multitask training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc339a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 14:23:09.582403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-04 14:23:09.582716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582803: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582831: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582887: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582914: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-01-04 14:23:09.582948: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-01-04 14:23:09.583191: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 2)       20000       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 784)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 1000)         0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " masked_dense (MaskedDense)     (None, 512)          803840      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " masked_dense_1 (MaskedDense)   (None, 512)          1025024     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " multi_masked_dense (MultiMaske  [(None, 256),       525312      ['masked_dense[0][0]',           \n",
      " dDense)                         (None, 256)]                     'masked_dense_1[0][0]']         \n",
      "                                                                                                  \n",
      " multi_masked_dense_1 (MultiMas  [(None, 256),       263168      ['multi_masked_dense[0][0]',     \n",
      " kedDense)                       (None, 256)]                     'multi_masked_dense[0][1]']     \n",
      "                                                                                                  \n",
      " multi_masked_dense_2 (MultiMas  [(None, 256),       263168      ['multi_masked_dense_1[0][0]',   \n",
      " kedDense)                       (None, 256)]                     'multi_masked_dense_1[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_3 (MultiMas  [(None, 256),       263168      ['multi_masked_dense_2[0][0]',   \n",
      " kedDense)                       (None, 256)]                     'multi_masked_dense_2[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_4 (MultiMas  [(None, 256),       263168      ['multi_masked_dense_3[0][0]',   \n",
      " kedDense)                       (None, 256)]                     'multi_masked_dense_3[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_5 (MultiMas  [(None, 256),       263168      ['multi_masked_dense_4[0][0]',   \n",
      " kedDense)                       (None, 256)]                     'multi_masked_dense_4[0][1]']   \n",
      "                                                                                                  \n",
      " selector_layer (SelectorLayer)  (None, 256)         0           ['multi_masked_dense_5[0][0]',   \n",
      "                                                                  'multi_masked_dense_5[0][1]']   \n",
      "                                                                                                  \n",
      " selector_layer_1 (SelectorLaye  (None, 256)         0           ['multi_masked_dense_5[0][0]',   \n",
      " r)                                                               'multi_masked_dense_5[0][1]']   \n",
      "                                                                                                  \n",
      " masked_dense_2 (MaskedDense)   (None, 10)           5140        ['selector_layer[0][0]']         \n",
      "                                                                                                  \n",
      " masked_dense_3 (MaskedDense)   (None, 1)            514         ['selector_layer_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,695,670\n",
      "Trainable params: 1,857,835\n",
      "Non-trainable params: 1,837,835\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the input block for the fashion data, which includes an input layer, a flatten layer, and a masked dense layer\n",
    "fashion_input = tf.keras.layers.Input(fashion_x_train.shape[1:])\n",
    "fashion_flatten = tf.keras.layers.Flatten()(fashion_input)\n",
    "fashion_reshape = mann.layers.MaskedDense(512, activation = 'relu')(fashion_flatten)\n",
    "\n",
    "# Create the input block for the reviews data, which includes an input layer, an embedding layer, a flatten layer,\n",
    "# and a masked dense layer of equal output shape to the masked dense layer for the fashion input block\n",
    "imdb_input = tf.keras.layers.Input(imdb_x_train.shape[1:])\n",
    "imdb_embedding = tf.keras.layers.Embedding(10000, 2)(imdb_input)\n",
    "imdb_flatten = tf.keras.layers.Flatten()(imdb_embedding)\n",
    "imdb_reshape = mann.layers.MaskedDense(512, activation = 'relu')(imdb_flatten)\n",
    "\n",
    "# Now that the shapes align for each of the tasks, we can push the data through multitask layers\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')([fashion_reshape, imdb_reshape])\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "\n",
    "# Output block for the fashion data\n",
    "fashion_selector = mann.layers.SelectorLayer(0)(x)\n",
    "fashion_output = mann.layers.MaskedDense(10, activation = 'softmax')(fashion_selector)\n",
    "\n",
    "# Output block for the IMDB data\n",
    "imdb_selector = mann.layers.SelectorLayer(1)(x)\n",
    "imdb_output = mann.layers.MaskedDense(1, activation = 'sigmoid')(imdb_selector)\n",
    "\n",
    "# Instantiate the model and compile it\n",
    "model = tf.keras.models.Model([fashion_input, imdb_input], [fashion_output, imdb_output])\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam'\n",
    ")\n",
    "\n",
    "# Mask (prune) the model and recompile for training\n",
    "model = mann.utils.mask_model(\n",
    "    model,\n",
    "    90,\n",
    "    x = [fashion_x_train[:1000], imdb_x_train[:1000]],\n",
    "    y = [fashion_y_train[:1000], imdb_y_train[:1000]]\n",
    ")\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a99c78",
   "metadata": {},
   "source": [
    "## Train the Model on the First Task\n",
    "\n",
    "After the model has been prepared, we can train it.  As stated previously, we are training the model in sequence on the individual tasks.  To do so, we compile the model so as to only optimize the loss for the specific task that is currently being trained, then pass dummy data to the task which is not being trained (in this case, empty-valued arrays).  The pruning previously done ensures only task-specific weights are being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15c2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "94/94 - 4s - loss: 1.8025 - masked_dense_2_loss: 1.8025 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.2560 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 1.1106 - val_masked_dense_2_loss: 1.1106 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.4996 - val_masked_dense_3_accuracy: 0.0000e+00 - 4s/epoch - 40ms/step\n",
      "Epoch 2/100\n",
      "94/94 - 2s - loss: 1.0035 - masked_dense_2_loss: 1.0035 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.5629 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.9581 - val_masked_dense_2_loss: 0.9581 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.5941 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 24ms/step\n",
      "Epoch 3/100\n",
      "94/94 - 2s - loss: 0.9133 - masked_dense_2_loss: 0.9133 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.6152 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.8694 - val_masked_dense_2_loss: 0.8694 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.6458 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 24ms/step\n",
      "Epoch 4/100\n",
      "94/94 - 2s - loss: 0.8446 - masked_dense_2_loss: 0.8446 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.6576 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.8169 - val_masked_dense_2_loss: 0.8169 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.6774 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 5/100\n",
      "94/94 - 2s - loss: 0.8046 - masked_dense_2_loss: 0.8046 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.6796 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.7721 - val_masked_dense_2_loss: 0.7721 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.6897 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 6/100\n",
      "94/94 - 2s - loss: 0.7662 - masked_dense_2_loss: 0.7662 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7010 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.7519 - val_masked_dense_2_loss: 0.7519 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7066 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 7/100\n",
      "94/94 - 2s - loss: 0.7272 - masked_dense_2_loss: 0.7272 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7267 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.7153 - val_masked_dense_2_loss: 0.7153 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7269 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 8/100\n",
      "94/94 - 2s - loss: 0.7146 - masked_dense_2_loss: 0.7146 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7340 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.6914 - val_masked_dense_2_loss: 0.6914 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7347 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 9/100\n",
      "94/94 - 2s - loss: 0.6705 - masked_dense_2_loss: 0.6705 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7532 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.6619 - val_masked_dense_2_loss: 0.6619 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7591 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 10/100\n",
      "94/94 - 2s - loss: 0.6456 - masked_dense_2_loss: 0.6456 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7641 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.6345 - val_masked_dense_2_loss: 0.6345 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7718 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 11/100\n",
      "94/94 - 2s - loss: 0.6120 - masked_dense_2_loss: 0.6120 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7778 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.6006 - val_masked_dense_2_loss: 0.6006 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7845 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 12/100\n",
      "94/94 - 2s - loss: 0.5749 - masked_dense_2_loss: 0.5749 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7882 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.5731 - val_masked_dense_2_loss: 0.5731 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7897 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 13/100\n",
      "94/94 - 2s - loss: 0.5403 - masked_dense_2_loss: 0.5403 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.7989 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.5420 - val_masked_dense_2_loss: 0.5420 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.7970 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 14/100\n",
      "94/94 - 2s - loss: 0.4973 - masked_dense_2_loss: 0.4973 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8178 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.5044 - val_masked_dense_2_loss: 0.5044 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8230 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 15/100\n",
      "94/94 - 2s - loss: 0.4718 - masked_dense_2_loss: 0.4718 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8326 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4743 - val_masked_dense_2_loss: 0.4743 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8378 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 16/100\n",
      "94/94 - 2s - loss: 0.4490 - masked_dense_2_loss: 0.4490 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8425 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4597 - val_masked_dense_2_loss: 0.4597 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8407 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 17/100\n",
      "94/94 - 2s - loss: 0.4283 - masked_dense_2_loss: 0.4283 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8499 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4479 - val_masked_dense_2_loss: 0.4479 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8455 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 18/100\n",
      "94/94 - 2s - loss: 0.4151 - masked_dense_2_loss: 0.4151 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8537 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4533 - val_masked_dense_2_loss: 0.4533 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8445 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 19/100\n",
      "94/94 - 2s - loss: 0.3987 - masked_dense_2_loss: 0.3987 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8591 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4308 - val_masked_dense_2_loss: 0.4308 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8532 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 20/100\n",
      "94/94 - 2s - loss: 0.3887 - masked_dense_2_loss: 0.3887 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8636 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4219 - val_masked_dense_2_loss: 0.4219 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8543 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 21/100\n",
      "94/94 - 2s - loss: 0.3822 - masked_dense_2_loss: 0.3822 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8660 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4184 - val_masked_dense_2_loss: 0.4184 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8556 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 22/100\n",
      "94/94 - 2s - loss: 0.3757 - masked_dense_2_loss: 0.3757 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8668 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4224 - val_masked_dense_2_loss: 0.4224 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8533 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 23/100\n",
      "94/94 - 2s - loss: 0.3731 - masked_dense_2_loss: 0.3731 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8682 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.4067 - val_masked_dense_2_loss: 0.4067 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8588 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 24/100\n",
      "94/94 - 2s - loss: 0.3594 - masked_dense_2_loss: 0.3594 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8734 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.3967 - val_masked_dense_2_loss: 0.3967 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8618 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 25/100\n",
      "94/94 - 2s - loss: 0.3578 - masked_dense_2_loss: 0.3578 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8727 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.3972 - val_masked_dense_2_loss: 0.3972 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8632 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 26/100\n",
      "94/94 - 2s - loss: 0.3417 - masked_dense_2_loss: 0.3417 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8792 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.3959 - val_masked_dense_2_loss: 0.3959 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8650 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n",
      "Epoch 27/100\n",
      "94/94 - 2s - loss: 0.3403 - masked_dense_2_loss: 0.3403 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.8780 - masked_dense_3_accuracy: 0.0000e+00 - val_loss: 0.3989 - val_masked_dense_2_loss: 0.3989 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.8625 - val_masked_dense_3_accuracy: 0.0000e+00 - 2s/epoch - 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f593e8d8340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell compiles the model for training task 1 (fashion) and trains the model for that task\n",
    "callback = tf.keras.callbacks.EarlyStopping(min_delta = 0.01, patience = 3, restore_best_weights = True)\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam',\n",
    "    loss_weights = [1, 0]\n",
    ")\n",
    "model.fit(\n",
    "    [fashion_x_train, np.zeros((fashion_x_train.shape[0], imdb_x_train.shape[1]))],\n",
    "    [fashion_y_train, np.zeros(fashion_y_train.shape)],\n",
    "    batch_size = 512,\n",
    "    epochs = 100,\n",
    "    callbacks = [callback],\n",
    "    validation_split = 0.2,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff0fc3",
   "metadata": {},
   "source": [
    "# Train the Model on the Second Task\n",
    "\n",
    "Once the model has been trained on the first task, we repeat the process to train on the second task, this time compiling the model such that only the second task is trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543ad980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 - 4s - loss: 0.6932 - masked_dense_2_loss: 2.1084 - masked_dense_3_loss: 0.6932 - masked_dense_2_accuracy: 0.0000e+00 - masked_dense_3_accuracy: 0.5016 - val_loss: 0.6932 - val_masked_dense_2_loss: 2.1084 - val_masked_dense_3_loss: 0.6932 - val_masked_dense_2_accuracy: 0.0000e+00 - val_masked_dense_3_accuracy: 0.4938 - 4s/epoch - 22ms/step\n",
      "Epoch 2/100\n",
      "157/157 - 2s - loss: 0.6569 - masked_dense_2_loss: 2.1084 - masked_dense_3_loss: 0.6569 - masked_dense_2_accuracy: 0.0000e+00 - masked_dense_3_accuracy: 0.5861 - val_loss: 0.5202 - val_masked_dense_2_loss: 2.1084 - val_masked_dense_3_loss: 0.5202 - val_masked_dense_2_accuracy: 0.0000e+00 - val_masked_dense_3_accuracy: 0.8172 - 2s/epoch - 14ms/step\n",
      "Epoch 3/100\n",
      "157/157 - 2s - loss: 0.3223 - masked_dense_2_loss: 2.1084 - masked_dense_3_loss: 0.3223 - masked_dense_2_accuracy: 0.0000e+00 - masked_dense_3_accuracy: 0.8741 - val_loss: 0.3091 - val_masked_dense_2_loss: 2.1084 - val_masked_dense_3_loss: 0.3091 - val_masked_dense_2_accuracy: 0.0000e+00 - val_masked_dense_3_accuracy: 0.8744 - 2s/epoch - 14ms/step\n",
      "Epoch 4/100\n",
      "157/157 - 2s - loss: 0.1829 - masked_dense_2_loss: 2.1084 - masked_dense_3_loss: 0.1829 - masked_dense_2_accuracy: 0.0000e+00 - masked_dense_3_accuracy: 0.9342 - val_loss: 0.3232 - val_masked_dense_2_loss: 2.1084 - val_masked_dense_3_loss: 0.3232 - val_masked_dense_2_accuracy: 0.0000e+00 - val_masked_dense_3_accuracy: 0.8748 - 2s/epoch - 14ms/step\n",
      "Epoch 5/100\n",
      "157/157 - 2s - loss: 0.1159 - masked_dense_2_loss: 2.1084 - masked_dense_3_loss: 0.1159 - masked_dense_2_accuracy: 0.0000e+00 - masked_dense_3_accuracy: 0.9617 - val_loss: 0.3956 - val_masked_dense_2_loss: 2.1084 - val_masked_dense_3_loss: 0.3956 - val_masked_dense_2_accuracy: 0.0000e+00 - val_masked_dense_3_accuracy: 0.8724 - 2s/epoch - 14ms/step\n",
      "Epoch 6/100\n",
      "157/157 - 2s - loss: 0.0749 - masked_dense_2_loss: 2.1084 - masked_dense_3_loss: 0.0749 - masked_dense_2_accuracy: 0.0000e+00 - masked_dense_3_accuracy: 0.9769 - val_loss: 0.4392 - val_masked_dense_2_loss: 2.1084 - val_masked_dense_3_loss: 0.4392 - val_masked_dense_2_accuracy: 0.0000e+00 - val_masked_dense_3_accuracy: 0.8720 - 2s/epoch - 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f593fae07f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell compiles the model for training task 2 (IMDB) and trains the model for that task\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam',\n",
    "    loss_weights = [0, 1]\n",
    ")\n",
    "model.fit(\n",
    "    [np.zeros((imdb_x_train.shape[0],) + fashion_x_train.shape[1:]), imdb_x_train],\n",
    "    [np.zeros(imdb_y_train.shape[0]), imdb_y_train],\n",
    "    batch_size = 128,\n",
    "    epochs = 100,\n",
    "    callbacks = [callback],\n",
    "    validation_split = 0.2,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50841fee",
   "metadata": {},
   "source": [
    "## Remove masks\n",
    "\n",
    "After training is complete, we can perform the optional `remove_layer_masks` function, which removes the extra masking tensors utilized only during the training process.  This removal reduces the number of weights within the network, thus further optimizing it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4393221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 2)       20000       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 784)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 1000)         0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " masked_dense (Dense)           (None, 512)          401920      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " masked_dense_1 (Dense)         (None, 512)          512512      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " multi_masked_dense (MultiDense  [(None, 256),       262656      ['masked_dense[0][0]',           \n",
      " )                               (None, 256)]                     'masked_dense_1[0][0]']         \n",
      "                                                                                                  \n",
      " multi_masked_dense_1 (MultiDen  [(None, 256),       131584      ['multi_masked_dense[0][0]',     \n",
      " se)                             (None, 256)]                     'multi_masked_dense[0][1]']     \n",
      "                                                                                                  \n",
      " multi_masked_dense_2 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_1[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_1[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_3 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_2[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_2[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_4 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_3[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_3[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_5 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_4[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_4[0][1]']   \n",
      "                                                                                                  \n",
      " selector_layer (SelectorLayer)  (None, 256)         0           ['multi_masked_dense_5[0][0]',   \n",
      "                                                                  'multi_masked_dense_5[0][1]']   \n",
      "                                                                                                  \n",
      " selector_layer_1 (SelectorLaye  (None, 256)         0           ['multi_masked_dense_5[0][0]',   \n",
      " r)                                                               'multi_masked_dense_5[0][1]']   \n",
      "                                                                                                  \n",
      " masked_dense_2 (Dense)         (None, 10)           2570        ['selector_layer[0][0]']         \n",
      "                                                                                                  \n",
      " masked_dense_3 (Dense)         (None, 1)            257         ['selector_layer_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,857,835\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,857,835\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simplified_model = mann.utils.remove_layer_masks(model)\n",
    "simplified_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7de341",
   "metadata": {},
   "source": [
    "## Get predictions and Report Performance\n",
    "\n",
    "Lastly, we retrieve the predictions on the test data from the model and report the performance of the model on each of the individual tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb3fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_preds = simplified_model.predict([fashion_x_test, np.zeros((fashion_x_test.shape[0], imdb_x_test.shape[1]))])[0].argmax(axis = 1)\n",
    "imdb_preds = (simplified_model.predict([np.zeros((imdb_x_test.shape[0],) + fashion_x_test.shape[1:]), imdb_x_test])[1].flatten() >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249a5eb",
   "metadata": {},
   "source": [
    "## Fashion Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763c5443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion Test Performance:\n",
      "\n",
      "\n",
      "[[809   3  44  47   1   1  86   0   9   0]\n",
      " [  1 959   3  32   2   0   2   0   1   0]\n",
      " [ 11   0 743  13 149   0  80   0   4   0]\n",
      " [ 20  10  17 891  40   0  18   0   4   0]\n",
      " [  0   2  94  39 813   1  47   0   4   0]\n",
      " [  0   0   0   0   0 954   0  25   4  17]\n",
      " [174   2 146  38 119   0 502   0  19   0]\n",
      " [  0   0   0   0   0  22   0 953   0  25]\n",
      " [  0   1   3   4   9  12  21   3 945   2]\n",
      " [  0   0   0   0   0  21   0  49   2 928]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80      1000\n",
      "           1       0.98      0.96      0.97      1000\n",
      "           2       0.71      0.74      0.72      1000\n",
      "           3       0.84      0.89      0.86      1000\n",
      "           4       0.72      0.81      0.76      1000\n",
      "           5       0.94      0.95      0.95      1000\n",
      "           6       0.66      0.50      0.57      1000\n",
      "           7       0.93      0.95      0.94      1000\n",
      "           8       0.95      0.94      0.95      1000\n",
      "           9       0.95      0.93      0.94      1000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fashion Test Performance:')\n",
    "print('\\n')\n",
    "print(confusion_matrix(fashion_y_test, fashion_preds))\n",
    "print('\\n')\n",
    "print(classification_report(fashion_y_test, fashion_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b26936",
   "metadata": {},
   "source": [
    "## IMDB Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "277e429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Test Performance:\n",
      "\n",
      "\n",
      "[[10768  1732]\n",
      " [ 1734 10766]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     12500\n",
      "           1       0.86      0.86      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('IMDB Test Performance:')\n",
    "print('\\n')\n",
    "print((confusion_matrix(imdb_y_test, imdb_preds)))\n",
    "print('\\n')\n",
    "print(classification_report(imdb_y_test, imdb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71a825",
   "metadata": {},
   "source": [
    "## Save and load the model\n",
    "\n",
    "An important part of being able to deploy models is the ability to save and load the models to disk.  We provide extra utilities to help save and load these models consistent with the `keras` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fc569a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 2)       20000       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 784)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 1000)         0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " masked_dense (Dense)           (None, 512)          401920      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " masked_dense_1 (Dense)         (None, 512)          512512      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " multi_masked_dense (MultiDense  [(None, 256),       262656      ['masked_dense[0][0]',           \n",
      " )                               (None, 256)]                     'masked_dense_1[0][0]']         \n",
      "                                                                                                  \n",
      " multi_masked_dense_1 (MultiDen  [(None, 256),       131584      ['multi_masked_dense[0][0]',     \n",
      " se)                             (None, 256)]                     'multi_masked_dense[0][1]']     \n",
      "                                                                                                  \n",
      " multi_masked_dense_2 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_1[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_1[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_3 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_2[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_2[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_4 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_3[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_3[0][1]']   \n",
      "                                                                                                  \n",
      " multi_masked_dense_5 (MultiDen  [(None, 256),       131584      ['multi_masked_dense_4[0][0]',   \n",
      " se)                             (None, 256)]                     'multi_masked_dense_4[0][1]']   \n",
      "                                                                                                  \n",
      " selector_layer (SelectorLayer)  (None, 256)         0           ['multi_masked_dense_5[0][0]',   \n",
      "                                                                  'multi_masked_dense_5[0][1]']   \n",
      "                                                                                                  \n",
      " selector_layer_1 (SelectorLaye  (None, 256)         0           ['multi_masked_dense_5[0][0]',   \n",
      " r)                                                               'multi_masked_dense_5[0][1]']   \n",
      "                                                                                                  \n",
      " masked_dense_2 (Dense)         (None, 10)           2570        ['selector_layer[0][0]']         \n",
      "                                                                                                  \n",
      " masked_dense_3 (Dense)         (None, 1)            257         ['selector_layer_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,857,835\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,857,835\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simplified_model.save('cv_and_nlp_model.h5')\n",
    "loaded_model = tf.keras.models.load_model('cv_and_nlp_model.h5', custom_objects = mann.utils.get_custom_objects())\n",
    "loaded_model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51acfa6005ffec5e74e71d844e0daa05d24ac78244a0bb1b7874b497027552e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
