{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70d6010",
   "metadata": {},
   "source": [
    "# Training a Single Model for Computer Vision and Natural Language Processing\n",
    "\n",
    "In this notebook, we train a single MANN model to perform both a computer vision task (image classification) as well as a natural language processing task (sentiment analysis).  For this notebook, we will use the MNIST Fashion dataset as well as the IMDB Sentiment Analysis dataset to train and test on.  Training will be done in a sequential manner, i.e. the network will first learn to perform image classification and then will learn to perform sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adf8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages required for the experiment\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646127f8",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Data\n",
    "\n",
    "Our first step in this demonstration will be to load the data.  We are using the TensorFlow functions to download and/or load the datasets into memory.\n",
    "\n",
    "The `Fashion MNIST` dataset, which can be found at [this link](https://github.com/zalandoresearch/fashion-mnist), contains 60,000 grayscale training images of clothing items from 10 different classes, each 28 pixels high and 28 pixels wide.  The test set contains 10,000 grayscale images from the same 10 classes and with the same shape.  The only preprocessing we will be doing is dividing all pixel values by 255 to ensure all values fall in the interval [0, 1].\n",
    "\n",
    "The `IMDB Large Movie Review` dataset, which can be found at [this link](https://ai.stanford.edu/~amaas/data/sentiment/), contains 25,000 \"highly polar\" movie reviews for training and 25,000 reviews for testing.  When loaded through the TensorFlow function, these reviews are already converted into sequences of integers, where each integer corresponds to an individual word.  For our purposes, we will restrict the vocabulary to 10,000 words, so only the top 10,000 words will be represented, and any other words will be left as \"out of vocabulary\" (OOV).  Once that is complete, we will preprocess the inputs so that only the first 500 words in each review are considered, truncated and padded from the rear of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(fashion_x_train, fashion_y_train), (fashion_x_test, fashion_y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(imdb_x_train, imdb_y_train), (imdb_x_test, imdb_y_test) = tf.keras.datasets.imdb.load_data(num_words = 10000)\n",
    "\n",
    "# Preprocess each of the input datasets. For the images, normalize each of the pixels to values between 0 and 1.\n",
    "# For the reviews, truncate and/or pad the lengths to 500 words each\n",
    "fashion_x_train = fashion_x_train/255\n",
    "fashion_x_test = fashion_x_test/255\n",
    "imdb_x_train = tf.keras.preprocessing.sequence.pad_sequences(imdb_x_train, maxlen = 500, padding = 'post', truncating = 'post')\n",
    "imdb_x_test = tf.keras.preprocessing.sequence.pad_sequences(imdb_x_test, maxlen = 500, padding = 'post', truncating = 'post')\n",
    "\n",
    "# Reshape the target data to having one column\n",
    "fashion_y_train = fashion_y_train.reshape(-1, 1)\n",
    "fashion_y_test = fashion_y_test.reshape(-1, 1)\n",
    "imdb_y_train = imdb_y_train.reshape(-1, 1)\n",
    "imdb_y_test = imdb_y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcf698",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "Now that the data has been loaded and preprocessed, we can create the model.  Because of the different input shapes for our inputs, we have to develop some dedicated input regions within the model.  After data flows through those regions, which are pruned along with the rest of the network and include only a flatten and single fully connected layer for the fashion data and an embedding and fully connected layer for the IMDB data, the bulk of the network consists of shared layers.  \n",
    "\n",
    "In these shared layers, the training procedure identifies disjoint subsets of weights to dedicate to each individual task.  By isolating these highly-pruned subnetworks and representing the network weights across an extra, task-specific dimension, the network is able to learn multiple tasks in parallel or in sequence without exhibiting any interference between tasks.\n",
    "\n",
    "After passing through the shared layers, there are dedicated output layers for each task.  These are required because of the different output shapes between tasks - the image classification task differentiates between 10 different classes, whereas the sentiment analysis task performs binary classification to identify positive and negative reviews.\n",
    "\n",
    "After the model is instantiated, we utilize the `mask_model` function to perform masking (aka pruning) of the model.  This identifies the isolated subnetworks to perform each individual task and prepares the network for multitask training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc339a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input block for the fashion data, which includes an input layer, a flatten layer, and a masked dense layer\n",
    "fashion_input = tf.keras.layers.Input(fashion_x_train.shape[1:])\n",
    "fashion_flatten = tf.keras.layers.Flatten()(fashion_input)\n",
    "fashion_reshape = mann.layers.MaskedDense(512, activation = 'relu')(fashion_flatten)\n",
    "\n",
    "# Create the input block for the reviews data, which includes an input layer, an embedding layer, a flatten layer,\n",
    "# and a masked dense layer of equal output shape to the masked dense layer for the fashion input block\n",
    "imdb_input = tf.keras.layers.Input(imdb_x_train.shape[1:])\n",
    "imdb_embedding = tf.keras.layers.Embedding(10000, 2)(imdb_input)\n",
    "imdb_flatten = tf.keras.layers.Flatten()(imdb_embedding)\n",
    "imdb_reshape = mann.layers.MaskedDense(512, activation = 'relu')(imdb_flatten)\n",
    "\n",
    "# Now that the shapes align for each of the tasks, we can push the data through multitask layers\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')([fashion_reshape, imdb_reshape])\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "x = mann.layers.MultiMaskedDense(256, activation = 'relu')(x)\n",
    "\n",
    "# Output block for the fashion data\n",
    "fashion_selector = mann.layers.SelectorLayer(0)(x)\n",
    "fashion_output = mann.layers.MaskedDense(10, activation = 'softmax')(fashion_selector)\n",
    "\n",
    "# Output block for the IMDB data\n",
    "imdb_selector = mann.layers.SelectorLayer(1)(x)\n",
    "imdb_output = mann.layers.MaskedDense(1, activation = 'sigmoid')(imdb_selector)\n",
    "\n",
    "# Instantiate the model and compile it\n",
    "model = tf.keras.models.Model([fashion_input, imdb_input], [fashion_output, imdb_output])\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam'\n",
    ")\n",
    "\n",
    "# Mask (prune) the model and recompile for training\n",
    "model = mann.utils.mask_model(\n",
    "    model,\n",
    "    90,\n",
    "    x = [fashion_x_train[:1000], imdb_x_train[:1000]],\n",
    "    y = [fashion_y_train[:1000], imdb_y_train[:1000]]\n",
    ")\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a99c78",
   "metadata": {},
   "source": [
    "## Train the Model on the First Task\n",
    "\n",
    "After the model has been prepared, we can train it.  As stated previously, we are training the model in sequence on the individual tasks.  To do so, we compile the model so as to only optimize the loss for the specific task that is currently being trained, then pass dummy data to the task which is not being trained (in this case, empty-valued arrays).  The pruning previously done ensures only task-specific weights are being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell compiles the model for training task 1 (fashion) and trains the model for that task\n",
    "callback = tf.keras.callbacks.EarlyStopping(min_delta = 0.01, patience = 3, restore_best_weights = True)\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam',\n",
    "    loss_weights = [1, 0]\n",
    ")\n",
    "model.fit(\n",
    "    [fashion_x_train, np.zeros((fashion_x_train.shape[0], imdb_x_train.shape[1]))],\n",
    "    [fashion_y_train, np.zeros(fashion_y_train.shape)],\n",
    "    batch_size = 512,\n",
    "    epochs = 100,\n",
    "    callbacks = [callback],\n",
    "    validation_split = 0.2,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff0fc3",
   "metadata": {},
   "source": [
    "# Train the Model on the Second Task\n",
    "\n",
    "Once the model has been trained on the first task, we repeat the process to train on the second task, this time compiling the model such that only the second task is trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ad980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell compiles the model for training task 2 (IMDB) and trains the model for that task\n",
    "model.compile(\n",
    "    loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = 'adam',\n",
    "    loss_weights = [0, 1]\n",
    ")\n",
    "model.fit(\n",
    "    [np.zeros((imdb_x_train.shape[0],) + fashion_x_train.shape[1:]), imdb_x_train],\n",
    "    [np.zeros(imdb_y_train.shape[0]), imdb_y_train],\n",
    "    batch_size = 128,\n",
    "    epochs = 100,\n",
    "    callbacks = [callback],\n",
    "    validation_split = 0.2,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50841fee",
   "metadata": {},
   "source": [
    "## Remove masks\n",
    "\n",
    "After training is complete, we can perform the optional `remove_layer_masks` function, which removes the extra masking tensors utilized only during the training process.  This removal reduces the number of weights within the network, thus further optimizing it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4393221",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_model = mann.utils.remove_layer_masks(model)\n",
    "simplified_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7de341",
   "metadata": {},
   "source": [
    "## Get predictions and Report Performance\n",
    "\n",
    "Lastly, we retrieve the predictions on the test data from the model and report the performance of the model on each of the individual tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_preds = simplified_model.predict([fashion_x_test, np.zeros((fashion_x_test.shape[0], imdb_x_test.shape[1]))])[0].argmax(axis = 1)\n",
    "imdb_preds = (simplified_model.predict([np.zeros((imdb_x_test.shape[0],) + fashion_x_test.shape[1:]), imdb_x_test])[1].flatten() >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249a5eb",
   "metadata": {},
   "source": [
    "## Fashion Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fashion Test Performance:')\n",
    "print('\\n')\n",
    "print(confusion_matrix(fashion_y_test, fashion_preds))\n",
    "print('\\n')\n",
    "print(classification_report(fashion_y_test, fashion_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b26936",
   "metadata": {},
   "source": [
    "## IMDB Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IMDB Test Performance:')\n",
    "print('\\n')\n",
    "print((confusion_matrix(imdb_y_test, imdb_preds)))\n",
    "print('\\n')\n",
    "print(classification_report(imdb_y_test, imdb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71a825",
   "metadata": {},
   "source": [
    "## Save and load the model\n",
    "\n",
    "An important part of being able to deploy models is the ability to save and load the models to disk.  We provide extra utilities to help save and load these models consistent with the `keras` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc569a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_model.save('cv_and_nlp_model.h5')\n",
    "loaded_model = tf.keras.models.load_model('cv_and_nlp_model.h5', custom_objects = mann.utils.get_custom_objects())\n",
    "loaded_model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51acfa6005ffec5e74e71d844e0daa05d24ac78244a0bb1b7874b497027552e7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
